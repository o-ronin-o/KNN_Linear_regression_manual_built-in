{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb54a083",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ Regression Task â€” Linear, Ridge, and Lasso Regression\n",
    "\n",
    "This section focuses on the **California Housing Prices** dataset to predict the **Median House Value** based on various demographic and geographic features.\n",
    "\n",
    "We will explore **three regression models**:\n",
    "\n",
    "1. **Linear Regression (Manual Implementation)**  \n",
    "   - Compute the optimal weights using the **Normal Equation**:  \n",
    "     \\( w = (X^T X)^{-1} X^T y \\)  \n",
    "   - Implement **Gradient Descent** as an alternative optimization method.\n",
    "\n",
    "2. **Regularized Regression Models**  \n",
    "   - **Ridge Regression (L2)**: adds a penalty on large weights to reduce overfitting.  \n",
    "   - **Lasso Regression (L1)**: encourages sparsity by shrinking some weights to zero.\n",
    "\n",
    "3. **Scikit-Learn Implementations**  \n",
    "   - Reapply the above models using `LinearRegression`, `Ridge`, and `Lasso` from `sklearn.linear_model`.\n",
    "\n",
    "We will analyze model performance using:\n",
    "- **Mean Squared Error (MSE)**\n",
    "- **Mean Absolute Error (MAE)**\n",
    "\n",
    "Finally, we will plot **Validation Error vs. Regularization Parameter (Î»)** and discuss the effects of regularization on bias-variance tradeoff.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
